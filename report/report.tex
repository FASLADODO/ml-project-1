\documentclass{article} % For LaTeX2e
% We will use NIPS submission format
\usepackage{nips13submit_e,times}
% for hyperlinks
\usepackage{hyperref}
\usepackage{url}
% For figures
\usepackage{graphicx}
\usepackage{subfigure}
% math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsopn}
\usepackage{ifthen}
\usepackage{natbib}

\title{Machine Learning Project I by Group KATHMANDU}

\author{
Jade Copet\\
EPFL \\
\texttt{jade.copet@epfl.com} \\
\And
Merlin Nimier David\\
EPFL \\
\texttt{merlin.nimier-david@epfl.com} \\
\And
Krishna Raj Sapkota\\
EPFL \\
\texttt{krishna.sapkota@epfl.com} \\
}

\nipsfinalcopy

\begin{document}
\maketitle



\begin{abstract}
  In this report, we summarize our findings for the project-I. We analyzed two datasets, regression (D1) and classification (D2). 
\end{abstract}



\section{The regression dataset (D1)}

  \subsection{Dataset description}
  Our training data consists of output variable $\mathbf{y}$ and input variables $\mathbf{X}$. We have $N = 1400$ data examples. Each input vector $\mathbf{x}_n$ is of dimensionality $D = 44$. Out of these 44 features, 35 are real-valued while 4 variables are binary and 5 variables are categorical, 4 of them with 4 categories and 1 with 3 categories.

  We also have test data where we do not observe $\mathbf{y}$. We have $N = 600$ test examples. Our goal is to produce predictions for test examples, as well as an approximation of the test error.

  \subsection{Data visualization and cleaning}
  We start by plotting the distribution of the input to obtain Figure \ref{fig:histY}. We notice a gaussian centered around 2500, but also 144 data points above 6500. It represents more than 10\% of our data example, thus they cannot be discarded as outliers: we make the hypothesis that our dataset includes two models.

  Since input variables are not normalized, we center and rescale them before going forward. Plotting each input variable against the output, we notice that feature 35 seems to allow us to separate linearly the two hypothetical models. The categorical variables do not help in an obvious way, as we could have hoped. We separate points roughly at $X_{35} > 1$ (after normalization) and plot the results in Figure \ref{fig:twoModelsRough}.
  
  We also investigated the correlation first between output and input variables and then between features themselves. On one hand, the variables  35 and 26 are very correlated to the output with correlation coefficients respectively equals to 0.67 and 0.43. The results using only those two variables were quite satisfying but not sufficient as too many features are put aside. On the other hand, a majority of features are almost uncorrelated to the output: for 34 of them the correlation coefficient is less than 0.1. However this does not suggest removal of all these variables yet as a combination of them might have a higher correlation to the output. Some features are highly correlated to each others: 8 pairs of features have a correlation coefficient higher than 0.5. From these two observations on regression between features and with the output, we tried to remove one feature of each highly correlated pairs which is not significantly correlated to the output. We finally kept the full input matrix  $\mathbf{X}$ for our regression fit as we have not faced any rank-deficiency issue and our results were satisfying keeping all the features.
  
  We use a dummy encoding for the categorical variables. The binary variables do not require dummy encoding. This gives us a total of 50 input variables.

  \begin{figure}[!t]
  \center
  \subfigure[Distribution of output $\mathbf{y}$. There are to many values above 6500 to discard them as outliers.]{
    \includegraphics[width=2.5in]{figures/regression/hist-Y.pdf}
    \label{fig:histY}
  }
  \hfill
  \subfigure[$X_{35}$ seems to enable us to linearly separate the two models simply]{
    \includegraphics[width=2.5in]{figures/regression/model-separation-X35.pdf}
    \label{fig:twoModelsRough}
  }
  \hfill
  \subfigure[First try at separating the two models using $X_{35} > 1$]{
    \includegraphics[width=4in]{figures/regression/model-separation-rough.pdf}
    \label{fig:twoModelsRough}
  }
  \caption{}
  \end{figure}


  \subsection{Regression fit}
  As we observed, our regression data seems to follow two different models. Therefore we are applying two regression models to fit our data.
  
  - one-variable model VS least squares VS ridge regression\\
  - RMSE errors\\
  - Discussion on overfitting, proportion selection, validation...

  \subsection{Feature transformations}
  - use ridge regression because of the resulting matrix singularity\\
  - compare different polynomial degrees (box plots, rmse values), selection and validation

  \subsection{Predictions}



\section{The classification dataset (D2)}
  We applied many of the same techniques to D2.

  \subsection{Dataset description}
  This dataset has $N = 1500$ data examples with dimensionality $D = 32$, as well as $N' = 1500$ test examples. Out of these 32 features, 29 are real-valued and 3 are categorical. Output $\mathbf{y}$ is a binary variable, which implies we are faced with a classification problem. 1052 examples belong to class $C_1$ (for which $y = 0$) and 448 examples belong to $C_2$. Like before, our goal is to produce predictions for test examples, as well as an approximation of the test error.

  \subsection{Data visualization and cleaning}
We performed basic exploratory data analysis on our data. As expected the data is not centered, and therefore we normalize the data. Spotting outliers on a classification dataset is not an easy role, we have assumed that data points are outliers if for any feature they are more than 10 standard deviations away from the median. This gives us a cleaned data set of 1469 examples.

For more convenience in the features manipulation, categorical variables are moved to the last columns of the matrix and we use dummy encoding for each of them. This gives us a total of 46 input variables.

  \subsection{Regression fit}

  \subsection{Feature transformations}

  \subsection{Predictions}



\section{Summary}

\subsubsection*{Acknowledgments}
 We would like to thank Emtiyaz Khan and the teaching assistants for creating this project which was a great opportunity for us to put in practice our knowledge of Machine Learning on a full study-case. We also would like to thank them for their precious advices during the project.
 
\subsubsection*{References}

\end{document}
