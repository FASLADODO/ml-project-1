\documentclass{article} % For LaTeX2e
% We will use NIPS submission format
\usepackage{nips13submit_e,times}
% for hyperlinks
\usepackage{hyperref}
\usepackage{url}
% For figures
\usepackage{graphicx}
\usepackage{subfigure}
% math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsopn}
\usepackage{ifthen}
\usepackage{natbib}

\title{Machine Learning Project I by Group KATHMANDU}

\author{
Jade Copet\\
EPFL \\
\texttt{jade.copet@epfl.com} \\
\And
Merlin Nimier David\\
EPFL \\
\texttt{merlin.nimier-david@epfl.com} \\
\And
Krishna Raj Sapkota\\
EPFL \\
\texttt{krishna.sapkota@epfl.com} \\
}

\nipsfinalcopy

\begin{document}
\maketitle



\begin{abstract}
  In this report, we summarize our findings for the project-I. We analyzed two datasets, regression (D1) and classification (D2).
\end{abstract}



\section{The regression dataset (D1)}

  \subsection{Dataset description}
    Our training data consists of output variable $\mathbf{y}$ and input variables $\mathbf{X}$. We have $N = 1400$ data examples. Each input vector $\mathbf{x}_n$ is of dimensionality $D = 44$. Out of these 44 features, 35 are real-valued while 4 variables are binary and 5 variables are categorical, 4 of them with 4 categories and 1 with 3 categories.

    We also have test data where we do not observe $\mathbf{y}$. We have $N = 600$ test examples. Our goal is to produce predictions for test examples, as well as an approximation of the test error.

  \subsection{Data visualization and cleaning}
    - y distribution (histogram) notice 2 different shapes (can't be outliers)\\
    - (Not rank deficient)\\
    - Normalization\\
    - Model separation : with the X against Y plots with data separated wrt X35 value. Explain why it should not be considered as outliers \\
    - Correlations : results in a table ? we might be able to remove some features (go to regressionFitSelectedFeatures)\\
    - Dummy variables encoding

  \subsection{Regression fit}
    - one-variable model VS least squares VS ridge regression\\
    - RMSE errors\\
    - Discussion on overfitting, proportion selection, validation...

  \subsection{Feature transformations}
    - use ridge regression because of the resulting matrix singularity\\
    - compare different polynomial degrees (box plots, rmse values), selection and validation

  \subsection{Predictions}



\section{The classification dataset (D2)}
  We applied many of the same techniques to D2.

  \subsection{Dataset description}
  This dataset has $N = 1500$ data examples with dimensionality $D = 32$, as well as $N' = 1500$ test examples. Out of these 32 features, 29 are real-valued and 3 are categorical. Output $\mathbf{y}$ is a binary variable, which implies we are faced with a classification problem. 1052 examples belong to class $C_1$ (for which $y = 0$) and 448 examples belong to $C_2$. Like before, our goal is to produce predictions for test examples, as well as an approximation of the test error.

  \subsection{Data visualization and cleaning}

  \subsection{Regression fit}

  \subsection{Feature transformations}

  \subsection{Predictions}



\section{Summary}

\subsubsection*{Acknowledgments}

\subsubsection*{References}

\end{document}
